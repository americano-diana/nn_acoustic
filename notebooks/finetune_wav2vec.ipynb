{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565b87a7",
   "metadata": {},
   "source": [
    "# Project:\n",
    "This notebook is being written by DPAG as part of the DFG-funded Research Project \"Resolving the cognitive and neural basis of affective sound-meaning associations\" under supervision of Dr. Arash Aryani, PostDoc researcher at FU Berlin.\n",
    "\n",
    "Goal: We aim to fine-tune existing auditory DNNs in order to model and predict arousal & valence ratings from non-words.\n",
    "\n",
    "## Models:\n",
    "Current Model being tested is the XLRS-53 version of the wav2vec2 large model\n",
    "\n",
    "Huggingface: https://huggingface.co/facebook/wav2vec2-large-xlsr-53\n",
    "\n",
    "Short description: This model is a transformer-based model that learned speech representations on unlabeled data.\n",
    "\n",
    "Why it's fitting for the project:\n",
    "\n",
    "+ Pre-trained on shorter speech units than phonemes, this should make it so it's better for recognizing non-words compared to other models\n",
    "+ There's literature on how the model layers effectively encode acoustic and phonetic information.\n",
    "\n",
    "## Dataset:\n",
    "Data utilized was gathered and consists of (data Arash sent me) - more TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1142c2",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec09e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blxck\\Desktop\\nn_acoustic\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import torchaudio\n",
    "import librosa\n",
    "from torch import nn\n",
    "import optuna\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm  # Library for progress bars\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split # For dataloader split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy import stats\n",
    "\n",
    "# Local functions from src\n",
    "import sys\n",
    "from pathlib import Path\n",
    "root = os.path.abspath(\"..\") # Go up to root folder\n",
    "\n",
    "if root not in sys.path:\n",
    "    sys.path.append(root)\n",
    "\n",
    "from src.load_data import load_data\n",
    "from src.task_utils import collate_fn, split_data\n",
    "from src.train_test import train_with_validation, test_model, test_stats, model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6849a17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "CUDA debugging enabled!\n",
      "Using device: CPU\n"
     ]
    }
   ],
   "source": [
    "# Set device = GPU // only needed for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Optional: enable more detailed CUDA error reporting\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "print(\"CUDA debugging enabled!\")\n",
    "print(f\"Using device: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98538aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set base dir with data files\n",
    "base_dir = Path(r\"C:\\Users\\blxck\\Desktop\\nn_acoustic\")\n",
    "data_dir = base_dir / \"data\" / \"processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543802f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Found 1103 matching audio-label pairs.\n",
      "\n",
      " 2. Loading audio files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading audio files:   0%|          | 0/1103 [00:00<?, ?it/s]c:\\Users\\blxck\\Desktop\\nn_acoustic\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "Loading audio files: 100%|██████████| 1103/1103 [00:15<00:00, 70.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " No normalization applied\n",
      "\n",
      "Data ready as a Torch object\n",
      "Total samples: 1103\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load wav files + labels\n",
    "\n",
    "model_sr = 16000 \n",
    "\n",
    "batch_sample = 1103 # This is the max size of shared files between wav_files and labels_df\n",
    "\n",
    "data = load_data(data_dir, batch_size=batch_sample, target_sr=model_sr)\n",
    "\n",
    "# Access dictionary variables\n",
    "waveforms = data[\"waveforms\"]\n",
    "valences = data[\"valences\"]\n",
    "arousals = data[\"arousals\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a8de35",
   "metadata": {},
   "source": [
    "### Define dataset and model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9d1ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLabelDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Creates a dataset class called \"Single Label Dataset\" that takes in the waveforms\n",
    "    and a chosen target (either valence or arousal)\n",
    "    \"\"\"\n",
    "    def __init__(self, waveforms, targets):\n",
    "        self.waveforms = waveforms\n",
    "        self.targets = targets  # Single label (valence or arousal)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.waveforms)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.waveforms[idx], torch.tensor(self.targets[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor, Wav2Vec2FeatureExtractor\n",
    "from datasets import load_dataset\n",
    "\n",
    " # Load pretrained model and processor\n",
    "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1905392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressor head with flexible architecture\n",
    "\n",
    "class OptimizedWav2Vec2Regression(nn.Module):\n",
    "    def __init__(self, base_model, hidden_size, dropout_rate, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.wav2vec2 = base_model\n",
    "\n",
    "        # Build dynamic regressor based on trial parameters\n",
    "        layers = []\n",
    "        input_size = self.wav2vec2.config.hidden_size\n",
    "\n",
    "        for i in range(num_layers - 1):\n",
    "            layers.extend([\n",
    "                nn.Linear(input_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            input_size = hidden_size\n",
    "\n",
    "        # Final output layer\n",
    "        layers.append(nn.Linear(input_size, 1))\n",
    "\n",
    "        self.regressor = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input_values, attention_mask=None):\n",
    "        outputs = self.wav2vec2(input_values, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state.mean(dim=1)\n",
    "        return self.regressor(pooled).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e7657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more complex regression model\n",
    "\n",
    "class ComplexWav2Vec2Regression(nn.Module):\n",
    "    def __init__(self, base_model, hidden_size, dropout_rate, num_layers=3, activation='gelu', use_batch_norm=True, pooling_method='mean', use_residual=True):\n",
    "        super().__init__()\n",
    "        self.wav2vec2 = base_model\n",
    "        self.pooling_method = pooling_method\n",
    "        self.use_residual = use_residual\n",
    "\n",
    "        # Activation selection\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'gelu': nn.GELU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'swish': nn.SiLU()\n",
    "        }\n",
    "        act_fn = activations.get(activation, nn.ReLU())\n",
    "\n",
    "        # Build regression layers\n",
    "        layers = []\n",
    "        input_size = self.wav2vec2.config.hidden_size\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(act_fn)\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_size = hidden_size\n",
    "\n",
    "        self.regressor = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(input_size, 1)\n",
    "\n",
    "        if use_residual:\n",
    "            self.residual_projection = nn.Linear(self.wav2vec2.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_values, attention_mask=None):\n",
    "        outputs = self.wav2vec2(input_values, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state.mean(dim=1) if self.pooling_method == 'mean' else outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.regressor(pooled)\n",
    "        out = self.output_layer(x).squeeze(1)\n",
    "        if self.use_residual:\n",
    "            out += self.residual_projection(pooled).squeeze(1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138cade",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cbb5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define params for model \n",
    "\n",
    "# Best found parameters for arousal through earlier optimization were from V2 Trial 5\n",
    "\n",
    "aro_params = {\n",
    "    \"learning_rate\": 1.0351315184578603e-05,\n",
    "    \"batch_size\": 4,\n",
    "    \"hidden_size\": 128,\n",
    "    \"dropout_rate\": 0.27555511385430487,\n",
    "    \"num_layers\": 2,\n",
    "    \"weight_decay\": 1.425475704402744e-05,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"scheduler\": \"step\",\n",
    "    \"criterion\": \"SmoothL1Loss\",\n",
    "    \"normalize_targets\": True,\n",
    "    \"grad_clip\": 1.5393422419156506,\n",
    "}\n",
    "\n",
    "# Create data loaders\n",
    "\n",
    "arousal_loaders = split_data(\n",
    "    waveforms,\n",
    "    arousals,\n",
    "    target_name=\"Arousal\", \n",
    "    batch_size=aro_params[\"batch_size\"], \n",
    "    collate_fn=collate_fn, \n",
    "    dataset_class=SingleLabelDataset)\n",
    "\n",
    "# Build custom model\n",
    "\n",
    "optimized_arousal_model = OptimizedWav2Vec2Regression(\n",
    "    base_model=Wav2Vec2Model.from_pretrained(model_name),\n",
    "    hidden_size=aro_params[\"hidden_size\"],\n",
    "    dropout_rate=aro_params[\"dropout_rate\"],\n",
    "    num_layers=aro_params[\"num_layers\"]\n",
    ").to(device)\n",
    "\n",
    "# Criterion\n",
    "criterion = torch.nn.SmoothL1Loss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    optimized_arousal_model.parameters(),\n",
    "    lr=aro_params[\"learning_rate\"],\n",
    "    weight_decay=aro_params[\"weight_decay\"]  # Adam supports weight_decay in PyTorch\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,   # same best aro params \n",
    "    gamma=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c6af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train arousal model\n",
    "\n",
    "aro_train_losses, aro_val_losses = train_with_validation(\n",
    "    model=optimized_arousal_model,\n",
    "    train_dataloader=arousal_loaders[\"train\"],\n",
    "    val_dataloader=arousal_loaders[\"val\"],\n",
    "    feature_extractor=feature_extractor,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    num_epochs=40,\n",
    "    sampling_rate=model_sr,\n",
    "    trial=None,\n",
    "    grad_clip=aro_params[\"grad_clip\"],\n",
    "    normalize_targets=aro_params[\"normalize_targets\"],\n",
    "    patience=3,\n",
    "    min_delta=1e-4,\n",
    "    variance_reg_coeff=0.1,\n",
    "    freeze_backbone_epochs=2,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae86c90",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0b9427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating on test set\n",
    "\n",
    "# Redefine criterion\n",
    "criterion = torch.nn.SmoothL1Loss()\n",
    "\n",
    "aro_test_loss, aro_r2, aro_mse, aro_mae, aro_preds, aro_targets = test_stats(\n",
    "    model=optimized_arousal_model,\n",
    "    test_dataloader=arousal_loaders[\"test\"],\n",
    "    feature_extractor=feature_extractor,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    sampling_rate=model_sr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc7c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison(aro_targets, aro_preds, r2=aro_r2, mse=aro_mse, mae=aro_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af07e859",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save final model - Optional\n",
    "# aro_full_model = \"/content/drive/MyDrive/Arash Projects/aro_full_model.pth\"\n",
    "# torch.save(optimized_arousal_model.state_dict(), aro_full_model)\n",
    "# print(f\"Retrained best arousal model saved at {aro_full_model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
